#the code of ResNet model for DC228455,there exist another one model
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tqdm import tqdm
import torchvision.models as models
import numpy as np
from sklearn.metrics import accuracy_score, roc_curve, auc, recall_score, f1_score, precision_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import label_binarize

# 定义设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 数据转换和增强
transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# 下载SVHN数据集
train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)
test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# 使用预训练的ResNet模型
model = models.resnet18(pretrained=True)

# 修改最后的全连接层以适应SVHN数据集的10个类别
model.fc = nn.Linear(model.fc.in_features, 10)

# 将模型移动到GPU（如果可用）
model = model.to(device)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

def train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=20):
    train_losses = []
    test_losses = []
    best_loss = float('inf')
    early_stopping_patience = 5
    patience_counter = 0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for images, labels in tqdm(train_loader):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * len(images)
        train_losses.append(running_loss / len(train_loader.dataset))

        model.eval()
        test_loss = 0.0
        all_labels = []
        all_preds = []
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                test_loss += loss.item() * len(inputs)
                _, preds = torch.max(outputs, 1)
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
            test_losses.append(test_loss / len(test_loader.dataset))

        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}")

        scheduler.step()

        # 提前停止
        if test_losses[-1] < best_loss:
            best_loss = test_losses[-1]
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print("Early stopping")
                break

    return train_losses, test_losses, all_labels, all_preds

# 训练模型20个epoch并收集损失曲线和预测结果
num_epochs = 20
train_losses, test_losses, all_labels, all_preds = train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs)

# Initialize the classifier for logistic regression comparison (using flattened image data)
X_train = train_loader.dataset.data.reshape(-1, 32*32*3) / 255.0  # Normalize to [0, 1]
y_train = train_loader.dataset.labels
X_test = test_loader.dataset.data.reshape(-1, 32*32*3) / 255.0   # Normalize to [0, 1]
y_test = test_loader.dataset.labels

clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)  # Increase max_iter if needed
clf.fit(X_train[:5000], y_train[:5000])  # Use a subset for faster training (adjust as needed)
y_pred_lr = clf.predict(X_test)
y_score_lr = clf.predict_proba(X_test)

# Accuracy for logistic regression model
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f"Logistic Regression Accuracy: {accuracy_lr:.2f}")

# Precision, Recall and F1 Score per class for logistic regression model
precision_lr = precision_score(y_test, y_pred_lr, average=None)
recall_lr = recall_score(y_test, y_pred_lr, average=None)
f1_lr = f1_score(y_test, y_pred_lr, average=None)
for i in range(10):
    print(f"Class {i}: Precision: {precision_lr[i]:.2f}, Recall: {recall_lr[i]:.2f}, F1 Score: {f1_lr[i]:.2f}")

# Confusion Matrix for logistic regression model
cm_lr = confusion_matrix(y_test, y_pred_lr)
print("Logistic Regression Confusion Matrix:")
print(cm_lr)
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr)
disp_lr.plot(cmap=plt.cm.Blues)
plt.title('Logistic Regression Confusion Matrix')
plt.show()

# Binarize the output for ROC and PR curves for logistic regression model
y_test_bin_lr = label_binarize(y_test, classes=range(10))
roc_auc_lr = dict()
for i in range(10):
    roc_auc_lr[i] = auc(*roc_curve(y_test_bin_lr[:, i], y_score_lr[:, i])[:2])
    print(f"Class {i}: Logistic Regression ROC AUC: {roc_auc_lr[i]:.2f}")

# Plot ROC curves for logistic regression model
for i in range(10):
    fpr_lr, tpr_lr, _ = roc_curve(y_test_bin_lr[:, i], y_score_lr[:, i])
    plt.plot(fpr_lr, tpr_lr, label=f'Class {i} (AUC={roc_auc_lr[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve')
plt.legend(loc='best')
plt.show()

# Plot the training and testing loss curves for ResNet model
plt.figure(figsize=(5, 5))
plt.plot(train_losses, label="Training Loss")
plt.plot(test_losses, label="Testing Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training and Testing Loss Curves")
plt.legend()
plt.show()

# Plot Precision-Recall curves
for i in range(10):
    precision_i, recall_i, _ = precision_recall_curve(y_test_bin_lr[:, i], y_score_lr[:, i])
    average_precision = average_precision_score(y_test_bin_lr[:, i], y_score_lr[:, i])
    plt.step(recall_i, precision_i, where='post', label=f'Class {i} (AP={average_precision:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='best')
plt.show()

# Decision boundary plot
x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure()
plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=20, edgecolor='k', cmap=plt.cm.RdYlBu)
plt.title('Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# 可视化一些测试结果
dataiter = iter(test_loader)
images, labels = next(dataiter)

# 打印图像和标签
print('GroundTruth: ', ' '.join(f'{labels[j]}' for j in range(4)))

outputs = model(images.to(device))
_, predicted = torch.max(outputs.cpu(), 1)

print('Predicted: ', ' '.join(f'{predicted[j]}' for j in range(4)))

# 显示图像
def imshow(img):
    img = img / 2 + 0.5
